\chapter{Problems in technical computing}

The original numerical computing language was Fortran, short for
``Formula Translating System'', released in 1957.  Since those
early days, scientists have dreamed of writing high-level, generic
formulas and having them translated automatically into low-level,
efficient machine code, tailored to the particular data types they
need to apply the formulas to.  Fortran made historic strides towards
realization of this dream, and its dominance in so many areas of
high-performance computing is a testament to its remarkable success.

The landscape of computing has changed dramatically over the years.
Modern scientific
computing environments such as MATLAB~\cite{matlab}, R~\cite{Rlang},
Mathematica~\cite{mathematica}, Octave~\cite{Octave},
Python (with NumPy)~\cite{numpy}, and SciLab~\cite{scilab} have grown in popularity
and fall under the general category
known as { {\it dynamic languages} or {\it dynamically typed languages}.
In these programming
languages, programmers write simple, high-level code without any
mention of types like \code{int}, \code{float} or \code{double} that
pervade {\it statically typed languages} such as  C and Fortran.



How should a computer scientist approach this space? We might try to
maximize performance. Or it is also easy to make unfounded assumptions about
``what users want''. But instead we should study the real world, see
what is happening and figure out how to steer it in a better direction.

Hypothesis: people don't know what they want. It's also hard to
predict what people will want in the future. We need, in the words
of Gerry Sussman, systems adaptable to uses not imagined by their
designers.

\section{What is a technical computing environment?}

This question has not really been answered.

Views of this are strongly shaped by what systems happen to exist,
and what people were exposed to as they learned to program.

Technical computing software has been designed haphazardly. Each system
is a pile of features taken without argument.


Some languages provide a ``convenient'' experience that is
qualitatively different from ``inconvenient'' languages. We believe
this can be made somewhat precise. A large part of it is reducing the
amount you need to know about any given piece of functionality in order
to use it.


These systems are function-oriented, typically providing a rather
large number of functions and a much smaller number of data types.

These functions, and the ways they are used, have a particular
character of being ``manifest'': one can just call them,
interactively, and see what they do. This notion includes the
following features:

\vspace{-3ex}
\begin{singlespace}
\begin{enumerate}
\item Performing fairly large tasks in one function
\item Minimal ``set up'' work to obtain suitable arguments
\item No surrounding declarations required
\item Permissiveness in accepting many data types and attempting to
      automatically handle as many cases as possible
\item Providing multiple related algorithms or behaviors under a single name
\end{enumerate}
\end{singlespace}

Language design choices affect the ability to provide this
user experience (though the first two items are also related to
library design). Informally, in order to provide
the desired experience a language needs to be able to assign a
meaning to a brief and isolated piece of code such as \texttt{sin(x)}.
This leads directly to making declarations and annotations optional,
eliding administrative tasks like memory management, and leaving
information implicit (for example the definition scopes and types of the
identifiers \texttt{sin} and \texttt{x}).
These characteristics are strongly associated
with the Lisp tradition of dynamically-typed, garbage collected
languages with interactive REPLs.

However, there are subtle cultural differences. A case in point is the
MATLAB \texttt{mldivide}, or backslash, operator \cite{matlabman:mldivide}.
By writing only \texttt{A\textbackslash B}, the user can solve square,
over- or under-determined linear systems that are dense or sparse, for multiple
data types. The arguments can even be scalars, in which case simple
division is performed. In short, a rather large amount of linear algebra
software is accessed via a single character! This contrasts with the
software engineering tradition, where clarifying programmer intent would
likely be considered more important. (TODO cite if possible?)
Even the Lisp tradition, which originated most of the convenience features
enumerated above, has sought to separate functionality into small pieces.
For example Scheme provides separate functions \texttt{list-ref} and
\texttt{vector-ref} \cite{schemelang} for indexing lists and vectors.



\section{Why dynamic typing?}

Mathematical abstractions often frustrate our attempts to represent them
on a computer. For example, mathematicians can move instinctively between
isomorphic objects such as scalars and 1-by-1 matrices, but most programming
languages would prefer to represent scalars and matrices quite differently.
A system might be able to convert automatically from one to the other, but
it cannot always know which one we \emph{meant}.


\subsection{Mismatches with mathematical abstractions}

Programs in general, deal with values of widely varying disjoint types:
functions, numbers, lists, network sockets, etc.
Type systems are good at sorting out values of these different types.
However, in mathematical code most values are numbers or number-like.
Numerical properties (such as positive, negative, even, odd, greater
than one, etc.) are what matter, and these are highly dynamic.

The classic example is square root (\texttt{sqrt}), whose result is complex
for negative arguments.
Including a number's sign in its type is a possibility, but this quickly gets
out of hand.
When computing eigenvalues, for example, the key property is matrix
symmetry.
Linear algebra provides many more examples where algorithm and data type
changes arise from subtle properties. These will be discussed in more
detail in section~\ref{sec:linalg}.

We must immediately acknowledge that static type systems provide
mechanisms for dealing with ``dynamic'' variations like these.


% as another
%example, in many cases it is convenient not to need to worry about
%integer overflow.


multiple common features underlie mathematical objects of different
types (e.g. numbers, sets, matrices). in some cases it makes sense to
consider numbers and matrices as the same kind of thing, and in other
cases it doesn't matter. A given type system is likely not to have
anticipated the particular common features that matter to your program,
making it more difficult to express an idea. A concrete example is
the matlab fragment
if condition
  idx = ':'
else
  idx = 1
end
where we want to select either an entire dimension or the first position
alone. The ':' and 1 are both indexes in this context, though they would
be of disjoint types in most programming languages.


\subsection{Operational reasoning}

people tend to think about programs operationally, i.e. what it *does* when
it runs. for example writing
if false
  code
end
the code does not "occur" and therefore does not need to be valid


there is less to learn. with static languages you have to learn what happens
at both compile time and run time, when only run time really matters.


there is a desire to parameterize as much as possible. functions
accept parameters, so function calling ought to be sufficient to express
any desired parameterization.


\subsection{I/O}

inevitably there is a need to refer to a datatype at run time. the best
example is file I/O, where you might want to say "read 500 double-precision
numbers from file X". in static languages the syntax and identifiers used
to specify such run-time types must be different from those used to specify
static types. in C you see defined constants such as \texttt{DATATYPE\_DOUBLE}.


\subsection{Flat metadata hierarchy}

static types are approximations of dynamic types, so languages with static
types inevitably assign two types to a location (both a static type and a
dynamic type) where one would do. in some languages, like C++, the desire
for performance or ease of implementation leads the compiler to make some
decisions based on static types. this is confusing. if type declarations
can be omitted, as in a type-inferred language, the situation is even worse
since the static type of a value might not be apparent.



\section{The software stack is too complex}

Collapsing abstraction layers

- for numerical debuggability in particular
- debuggability in general
- no time spent worrying about binding time
- you will build a dynamic dispatch layer anyway, so build it in


How much of the past 30 years of handwritten Matlab internals can
be autogenerated with a compiler? (A lot)

Can now get past traditional classifications of language boundaries
- interpreted vs. compiled, high-level vs. low-level, procedural vs.
imperative vs. functional, etc.

language performance psychology:
if your language doesn't directly support efficient machine data types,
users will rewrite their code in C in order to get them, and then be
happy with the result (though not with the process).

so julia is an all-levels language


\section{Code generation}

- tensor contraction engine
- Firedrake, pyop2 (vs. c++ libmesh)
- JuMP vs. python puLP and pyomo
  - want to be able to pass functions, not C++ code as strings
- FFTW
- erfinv and horner

\subsection{A compiler for every problem}


\section{Bad tradeoffs in current designs}


- Ducking the issue of typing altogether - why this isn't possible IRL

Case in point: Python and NumPy.

NumPy tried to work within the confines of Python, but has more or
less failed for technical and political reasons. (Technical: hard
to work with types in language that deliberately tries to obscure
them from the user. Political: unwillingness to fix broken package
distribution system.) Consequences: Numba and Numba\_lang, Anaconda.
Essentially reinventing types in ``Python''. 

This phenomenon is increasingly noticed in other domains, particularly
JavaScript and web programming. Modern JavaScript implementations are
quite fast, but Google's Dart language is based on the premise that
we could have a web language that is even faster, and offers more
productivity as well. How so? Because Dart's designers observed that
JavaScript programmers in practice often write code that could be
defined using traditional OO classes, but the language does not
support them.

dictionaries for everything (python, js) is the wrong default. almost every
type somebody wants has a fixed number of fields with fixed types.

Python is often described as a good glue language. This
means it is effectively used as an interface standard, a kind of
extended C ABI that makes it easier for libraries to interoperate,
and easier for users to access those libraries. Something as straightforward
as providing a standard N-dimensional numeric array class (NumPy),
which does not exist at the level of C, goes a long way.

However, we wish to point out that in this picture, Python is not
doing as much work as it might first appear. Python does not make
it easier to implement the functionality inside NumPy, or other
``under the hood'' scientific libraries. In many cases it creates
more work, through the need to write wrappers and interfaces
for native code.


Merely being ``dynamic'' (e.g. Python) should not be considered
the gold standard of flexibility. Although these systems permit
tricks that can solve otherwise difficult programming problems,
this is not always the kind of flexibility that is needed.
When faced with the need to describe many functions with elaborate
behavior and many cases, one does not primarily need permissiveness,
but rather powerful and descriptive organizing principles.



How does Julia break through the 4x-slower-than-C performance barrier?

bits types and parameters -- can have abstract types whose size is known
unconditionally




\subsection{What needs to be built in?}

On "built-in", why built-in is bad. Built-in-ness often conflates two
aspects:

\begin{enumerate}
\item A feature being readily available and agreed-on by all language users
\item A feature tightly coupled to the rest of the system
\end{enumerate}

(2) implies (1), but not the other way around. (2) is the only technically
interesting item, since the other can be addressed e.g. just by including
a library in the standard software distribution. Many technical computing
languages have done a large amount of (2) while justifying it with point (1).



While a large part of our motivation is to move more decisions and functionality
into libraries, it is equally important to identify what *must* be part of a
language for the system to be successful. We believe that large amounts of
functionality can be provided by add-ons, but that certain key features
absolutely cannot be. Past failures to properly classify features this way have
caused a lot of undue pain.

First, performance cannot be an add-on. If some users have a fast version of
a language and others have a slow version (with the difference being an
order of magnitude or more), library writers cannot be sure whether users
will find their code fast enough to be useful. How are we to teach people to
program in the language? Courses on MATLAB programming emphasize vectorized code,
but if not all implementations had this performance characteristic the
curriculum would become confused.

Psychologically, it may be difficult to accept a ``non standard'' extension
that changes a language so fundamentally. There is a nagging, though perhaps
totally unfounded, perception that something subtle may break. If indeed a
bug arises due to the use of such an extension, a user is likely to conclude
that the extension is dangerous or broken and stop using it. If, on the other
hand, a bug arises due to a language's standard optimizing compiler, the user
will simply file a bug report, then find a way to work around the problem.

Adding a JIT compiler to a language also requires acceptance of detriments
like compilation pauses and pages with RWX permissions. In some cases this
may lead to use of the extension being disallowed, perhaps for security
reasons.

Type systems similarly fail when provided as optional extensions. Library
writers face the same kinds of problems as with performance add-ons. Should I
use type annotations in my library?

Dynamic dispatch mechanisms also make especially poor add-ons. Of course,
every program makes decisions at run time, and so implements its own
``dispatch'' to some extent. But these behaviors are inextensible; if
language users do not agree on a reusable dispatch framework their code
will not be composable.


\section{Social phenomena}

Programming languages are observed to have strong network effects, and the
difficulty of getting new languages adopted is well known. However based on
[socio PLT paper] we believe this doesn't have to be the case. The formula
of improving or redesigning general-purposes languages to be more appealing to
domain experts might solve the problem. That way the new system has immediate
appeal for at least some users, without the worry that a different tool will
be needed as soon as requirements change slightly.

barrier to contributing

Software business is based on imposing restrictions


Debates about what abstractions mean -- AbstractMatrix
we thought we knew what it meant, but what about something like
SymTridiagonal? It can implement most of what a dense matrix
has, but it can't obey every invariant.

PackedQR
