\chapter{Introduction}

Scientific computing has evolved from being essentially the only kind of
computing that existed, to being a small part of how and why computers
are used.
Over this period of time, programming tools have advanced in terms of
the abstractions and generalizations they are capable of.

Science as a whole evolves through the incorporation of specialized bits
of knowledge into more powerful general theories.  
When we examine computer languages we see the parallel:  that
many special-purpose languages have been subsumed by languages that are more powerful and
general. 
How has this trend affected scientific computing?
The surprising answer is: not as much as we would like.
We see scientists and technical users generally either turning away
from the ``best'' new programming languages, or else pushing them
to their limits in unusual ways.

%% Much of the history of programming languages has been about increasing abstraction
%% and generalization. Today we take it for granted that a single suitably powerful
%% language could be used for nearly any programming task, were it not for pragmatic
%% factors beyond our control. Over time,
%% The field of technical computing --- programming for applied math and the
%% sciences --- is something of a holdout from this trend, with its own
%% purpose-built languages still dominating.
%% There are probably many reasons for this, including historical, social and
%% technical.

\begin{singlespace}
\begin{figure}
  \begin{center}
    \begin{tabular}{|llll|l|}\hline
      \multicolumn{4}{|c|}{Productivity} & Performance \\
      \hline
      Matlab  &  Maple &  Mathematica & SciPy & Fortress\\
      SciLab  &  IDL   &  R  & Octave         & Chapel \\
      S-PLUS  & SAS & J & APL                 & X10 \\
      Maxima & Mathcad & Axiom & Sage         & SAC \\
      Lush & Ch & LabView & O-Matrix          & ZPL \\
      PV-WAVE & Igor Pro & OriginLab & FreeMat &\\
      Yorick & GAUSS & MuPad & Genius &\\
      SciRuby & Ox & Stata & JLab &\\
      Magma & Euler & Rlab & Speakeasy &\\
      GDL & Nickle & gretl & ana &\\
      Torch7 & A+ & PDL & Nial & \\
      \hline
    \end{tabular}
  \end{center}
  \caption{
    49 technical computing languages classified as primarily designed for Productivity or Performance
  }
  \label{gangof40}
\end{figure}
\end{singlespace}

In fact, we have discovered at least \emph{49} programming languages
designed for technical computing since Fortran (figure~\ref{gangof40}).
This is a high number by any standard begging for  a reasonable hypothesis to
explain this high number.
We propose that users crave 
the flexibility to permit \emph{any} conceivable means of
describing computations.
Language design is then 
where you go when you need a lot
of this flexibility.
After all, language design is  associated with the highest level of abstraction.
The desire for scientists to rework their code  is not so surprising when one reflects on the tradition
of inventing new notation in mathematics and science.
%This certainly
%helps distinguish scientific programming from other application areas.

The need for language-level flexibility is corroborated by
the ways that the technical computing domain uses general purpose
languages.
Effective scientific libraries extensively employ
polymorphism, custom operators, and compile-time abstraction.
Code generation approaches (writing a program that writes the needed
program) are unusually common.
Most of these techniques are presently fairly difficult to use, and so
programmers working at this level give up the notational conveniences
of the purpose-built languages above.

%Yet it also demands the greatest ease-of-use.
%A good rule of thumb is that if you need to write a program that prints
%out code in language X, then language X is not powerful enough.

%pathologies arising from the language level are subtle
%not as immediate as ``this new matrix multiply is faster''


This thesis addresses the essential challenges created by
this state of affairs.
We argue that the situation   
can be improved considerably by
abstracting, with a new language.

%the essential challenges from the state
%f affairs we have described.
%Of course, a language is not the only thing the world of technical
%computing needs.
 Compiler \todo{see comment that follows}
techniques, library design, high-performance
computational kernels, new algorithms, and approaches to parallelism might
be as perhaps more important than language design.
However these sorts of technologies can usually be applied to multiple
languages, as has happened in the C and Fortran language families.
%, which have taken on many such extensions over the years.
So in our view languages targeting this space need to dig deeper,
at least temporarily forgetting about matrices and cache utilization,
and look for bigger patterns.
\todo[inline]{{\it Please consider if this is even more true: }\newline
Technical computing  can benefit from many technologies.
Some of these include compiler techniques, library design, high-performance
computational kernels, new algorithms, and new approaches to parallelism.
These sorts of technologies can usually be applied to multiple
languages, as has happened in the C and Fortran language families.
They do not by themselves insist that a new language be developed.
However in conjunction with a new language, these technologies can flourish.
Indeed, we believe that language design
has reached a level of importance that even (perhaps temporarily) supersedes
the traditional focus of cache utilization and the use of matrix operations.
}

In brief, our idea is to \emph{integrate code selection and specialization}.
The aforementioned flexibility requirement can be explained as
a sufficiently powerful means for \emph{selecting} which piece of code
to run. This notion subsumes method dispatch, function overloading,
and potentially even branches. When such mechanisms are used in
technical computing, there is nearly always a corresponding need to
\emph{specialize} code for specific cases to obtain performance.
Polymorphic languages sometimes support forms of specialization,
but often only through a designated mechanism (e.g. templates), or
deployed rather conservatively out of resource concerns.
We intend to show that an equilibrium point in the design
space can be found by combining selection and specialization
into a single dynamic multiple dispatch mechanism.
The specific design is identified via
subtyping theories and closure under data-flow operations.

%what does this system achieve?
%-

% why dynamic dispatch?
%% somewhat counter-intuitively, dynamic dispatch can be good for performance
%% since it permits invoking the most specialized possible method.
%% static overloading can lead to calling a sub-optimal case when multiple
%% overloads exist for the sake of performance.




%Extensive code specialization is a key feature of
%technical computing. ``what to specialize on'' has been an open problem.
%our types are a possible solution to this for two reasons:

%\begin{enumerate}
%\item you can tune the amount of information they contain
%\item everybody agrees to use them, which helps ensure that specializing on
%  the types will actually do something useful.
%\end{enumerate}

%the amazing thing about programming languages it that a better
%explanation can directly lead to better performance!)



%By \emph{selection} we mean any mechanism used to pick one of several
%pieces of code to run. This includes object-oriented mechanisms, as well
%as function overloading, and even branches.

%By \emph{specialization} we mean transformation of a given piece of code,
%particularly as done by a compiler to generate efficient code for some
%special case of a flexible piece of high-level code.

%% Specialization entails program analysis. By necessity, a language's
%% code selection mechanisms must inform this process, to ensure the
%% correctness of the analysis. A central argument of this thesis is that
%% specialization should feed back into selection, allowing the
%% \emph{approximate values} processed by a compiler to be used in the
%% source language for dispatch. We argue that this is the key feature
%% missing from the present generation of dynamically-typed languages.
%This conclusion would not be surprising to advocates of static
%type systems, but the approach we propose is actually a run-time
%mechanism that does not restrict the class of valid programs.

%Why integrate code specialization and code selection?
%\begin{itemize}
%\item specialization requires selection anyway
%\item simpler system
%\item can sometimes collapse 2 layers of dispatch into 1
%\item can replace library code with a code generator without either changing
%  client code *or* any extra overhead
%\end{itemize}

%From this perspective, the language used is
%incidental (``mere choice of notation''), which helps explain
%the proliferation of languages --- any language that can call BLAS
%\cite{blas} will do!

%What do we mean by a \emph{language} for scientific computing?
%The prevailing
%answer is that it is about numeric arrays, perhaps matrices specifically,
%and numerical libraries that use them.
%Instead we intend to argue that, at a
%deeper level, it is about certain kinds of flexibility, particularly
%flexibility in the behavior of key operators and functions. This flexibility
%is needed to maximize composability and reusability in scientific code.
%Without it, certain programs may be easy to write today, but changing
%functional and performance requirements can become difficult to meet.


\section{The technical computing problem}

\begin{singlespace}
\begin{figure}
  \begin{center}
    \def\arraystretch{1.25}
    \begin{tabular}{|l|l|}\hline
      \textbf{Mainstream PL} & \textbf{Technical computing} \\
      \hline \hline
      classes, single dispatch             &  complex operators \\
      \hline
      separate compilation                 &  performance, inlining \\
      \hline
      parametric polymorphism              &  ad-hoc polymorphism, extensibility \\
      % - concrete reuse of generated code
      \hline
      static checking                      &  experimental computing \\
      \hline
      modularity, encapsulation            &  large vocabularies \\
      \hline
      eliminating tags                     &  self-describing data, acceptance of tags \\
      \hline
      data hiding                          &  explicit memory layout \\
      \hline
    \end{tabular}
  \end{center}
  \caption{
    Priorities of mainstream object-oriented and functional programming language research and
    implementation compared to those of the technical computing domain.
  }
  \label{PLpriorities}
\end{figure}
\end{singlespace}

Figure~\ref{PLpriorities} compares the general design priorities of mainstream programming
languages to those of technical computing languages. The priorities in each row are not
necessarily opposites or even mutually exclusive, but rather are a matter of emphasis.
It is certainly possible to have both parametric and ad-hoc polymorphism within
the same language, but syntax, recommended idioms, and the design of the standard library will
tend to emphasize one or the other. Of course, the features on the left side can
also be useful for technical computing; we exaggerate to help make the point.

It is striking how different these priorities are. We believe these technical factors have
contributed significantly to the persistence of specialized environments in this area.
Imagine you want just the features on the left. Then there are many good
languages available: Haskell, ML, Java, C\#, perhaps C++.
Getting everything on the right, by comparison, always seems to be awkward.
The most popular approach is to use multiple languages, as in e.g. NumPy~\cite{numpy},
with a high-level productivity language layered on top of a large library
written in lower-level languages. Seeking a similar tradeoff,
others have gone as far as writing a C++ interpreter \cite{vasilev2012cling}.



%technical computing systems have an unusually large amount of
%``failure of abstraction'' --- manual duplication of facts
%all over the system. imagine changing from column-major to
%row-major.

%problem of finding connections between array programming and OOP.
%array programming is a powerful paradigm for describing computational
%kernels operating over potentially large amounts of data.

%an ``object system'' in this context is often considered a separate
%part of the language, to be used only when arrays no longer
%suffice.

\section{Solution space}

It is clear that any future scientific computing language will need to be able to
match the performance of C, C++, and Fortran. To do that, it is almost certain
that speculative optimizations such as tracing \cite{tracingjit} will not be sufficient ---
the language will need to be able to \emph{prove} facts about types, or at least
let the user request specific types. It is also clear that such a language must
strive for maximum convenience, or else the split between performance languages
and productivity languages will persist.

It is fair to say that two approaches to this problem are being tried: one is
to design better statically-typed languages, and the other is to apply
program analysis techniques to dynamically-typed languages.
Static type systems are getting close to achieving the desired level
of flexibility (as in Fortress \cite{fortresspec} or Polyduce \cite{polyduce1},
for instance), but it is still too early to call a winner between these two
approaches (if, indeed, there even needs to be a winner).

Here we will use a dynamically-typed approach, with approximate
type inference
(for an excellent discussion of the many meanings of the word ``type''
see \cite{Kell2014}).
There are several reasons for this.
First, we want to emphasize that insufficient static
checking is most likely not the current limiting factor in scientific
computing productivity. Second, some idioms in this domain appear to be
inherently, or at least naturally, dynamically-typed (as we will explore in
later \todo{Put in a ref to these chapters by number  please.} chapters).  Third, there has been a sense that ``dynamic'' or
``scripting'' language users do not want to hear about types: they are 
thought of as a distraction and 
associated with verbosity and nuisance compiler errors. We hope to
contribute an example of a language where types are useful and not
burdensome.

%Finally, it is often the case that a seemingly-minor
%modification to a type system makes type checking undecidable. Rejecting
%such a system is totally reasonable. However if we are willing to
%accept undecidable checking from the beginning, opportunities arise to
%simplify and increase the power of the language.
%Occasionally one hears that ``dynamic type'' is an oxymoron, or that
%these types are not ``real types''

% mention value of having types that are agreed-on and work well within
% the language. enables so many optimizations.

%Efforts to analyze and optimize dynamically-typed programs generally make two
%assumptions: (1) we should work on analyzing \emph{existing} popular
%languages, and (2) users of these languages don't want to use types.
%The first assumption makes practical sense.
%Convenience is hard to quantify, so using existing languages that have already
%been deemed convenient by popular opinion puts us on solid footing.
%This thesis describes a new language, so we simply take the negation of the
%first assumption as a premise.
%Our work addresses the second assumption more directly.
%We point out that types
%\footnote{}
%do not have to be used for static checking, and that using them for
%\emph{code selection} and \emph{code specialization} is particularly useful in
%technical computing.
%This perspective has not been explored thoroughly in the past.

\subsection{The excess power problem}

We claim that the amount of information that can be statically inferred
exceeds most dynamic languages' capacity to exploit it.
Much work has been done on program analysis and optimization techniques
for dynamically-typed languages.
When static analyses (often incorporating run-time information) are applied
to dynamically-typed programs, it is typically possible to recover a
significant amount of type information (TODO cite). \todo{cite} What, then, can one
do with this information? If the goal is performance, various partial
evaluations can be done: generating code without type checks, removing
branches, type-specializing the storage of variables, and compile-time
method lookup are all valuable and yield large real-world gains.

 For example,
if method calls are dispatched on the first argument, but the types of all
arguments can be inferred, some power has been ``left on the table'' ---
we could have had multi-methods for little extra cost. In fact, method-at-a-time
JIT compilers (TODO cite) \todo{cite}  can specialize method bodies on all arguments,
and might use multiple dispatch \emph{internally} to select implementations
at run time (TODO \todo{cite} cite a system that did this). This argument does not
apply equally to statically-typed languages, since they cannot simply
``switch'' their functions to generic functions without significant
consequences for type checking.

This ``excess power'' problem applies to data structures as well.
For example, static or run time analysis might reveal that a certain array
can be represented as a native \texttt{Int32} array \cite{Bolz2013}.
If this information is not reflected in the source language, then
certain uses like passing data to native code become unnecessarily more
complicated.
And if one is going to implement homogeneous arrays anyway, why not
let programmers request them?
%Of course, a library like NumPy \emph{does} allow this, but with a
%mechanism

Some levels of performance are difficult to reach with implicitly
specialized code and data. Given the knowledge that
an array contains only \texttt{Int32} data, we might want to go
beyond essential optimizations like storing intermediate values in
registers, and actually use different algorithms. For example,
in Miller-Rabin primality testing, checking three ``witness'' values
suffices for all 32-bit arguments, but up to seven values might be
needed for 64-bit arguments .\todo{cite} (TODO cite)
In cryptographic applications, exploiting this difference in an inner loop
could bring significant benefits.


\subsection{The divergence problem}

Another problem that occurs when analyzing programs with complex
type behavior is divergence: the analysis is likely to infer an
overly-large result from failing to eliminate enough possible
behaviors. Narrowing the inferred types requires some extra source
of type information. Multi-method signatures work well for this
purpose.

However the divergence problem also places a limit on what
kinds of dispatch specifications are useful to program analyses:
some sets of values tend to be more robust under execution
than others.
%Dispatch expressiveness can always be increased
%(e.g. including arbitrary predicates), but

% TODO:
% claim dispatch works better on approximate type info than branches
% when you have unionall types.
% dispatch is also extensible and declarative.

% doing analysis only, you can use arbitrarily complex types, but
% you are then limited by what the language can ``consume'', or
% by the sophistication of the t-functions you write.
% adding dispatch, the types become an abstraction under user
% control, so need to be meaningful.

%% what to dispatch on? dispatch power has been extended in many ways, but
%% there is no real limit to what somebody might want to dispatch on.
%% so what to do?
%% some sets of values are more robust under computation than others
%% (closure properties).
%% identify those sets using dataflow concerns.

%% say we have a method defined for integers, and also for the special cases
%% ``2'' and ``odd integers''. a realistic implementation
%% will group all of these under ``integer'', and ideally generate a couple
%% branches to handle the other cases. we argue the concept of ``integer''
%% here is a more robust set, and so a more fundamental language concept.

%- vs predicate dispatch: we extend dispatch power in a different direction,
%guided by semantic subtyping. goal is maximum power that still yields high
%likelihood of resolving many calls to a single implementation.


% on the ``sufficiently smart compiler''
% optimizing code is uncomputable in general, and in some sense
% AI-complete. as soon as a person names any *specific* optimization
% they want, it immediately becomes possible to imagine a compiler
% doing it.
% discussion on when dynamic typing is ``needed'' is related!
% again, isolated examples often unconvincing.
% same thing yet again tends to happen with multiple dispatch:
% it feels very useful overall, but many particular cases can be
% explained away as something single dispatch could handle.

\subsection{Staged programming}

In demanding applications, selecting the right algorithm might not
be enough, and we might need to automatically \emph{generate} code
to handle different situations. While these cases are relatively rare
in most kinds of programming, they are remarkably common in technical
computing. Code generation, also known as \emph{staged programming},
raises several complexities:

\vspace{-3ex}
\begin{singlespace}
\begin{itemize}
\item What is the input to the code generator?
\item When and how is code generation invoked?
\item How is the generated code incorporated into an application?
\end{itemize}
\end{singlespace}

\noindent
These questions lead to practical problems like ad-hoc custom syntax,
extra build steps, and excessive development effort (writing parsers and
code generators might take more work than the core problem).
If code needs to be generated based on run-time information, then a
% TODO cite julia GPU paper that did this
staged library might need to implement its own dispatch mechanism
for invoking its generated code.

We find that our particular style of dynamic multiple dispatch provides
an unusually good substrate for staged programming.
The reason is that the language, though semantically dynamically-typed,
has a canonical (and not optional!) type analysis stage.
This is a natural place to hook in code generators: semantic (not just
syntactic) information is available, but machine code has not been
generated yet.
In general, this semantic information can also include run-time (and
always, at least, approximate run-time) information.
Since the types used by dispatch have rich, nested structure, they
convey enough information to drive code generation for a significant
class of problems.
In this framework code generators are invoked automatically by
the compiler, and the results can be called directly or even inlined
into user code.
%A library can switched to a staged approach without affecting
%calling code.
Our approach is closely related to exotypes
\cite{DeVito:2014:FRG:2594291.2594307}.



%past (static) type systems for dispatch were designed to ensure the absence of
%no-method errors and ambiguities (completeness and uniqueness). our goal
%is instead to statically resolve methods. this is inherently heuristic and
%best-effort. since static types shouldn't affect program behavior, we
%conclude that the dispatch must be dynamic, which is happily the same
%conclusion you would reach if you simply wanted dynamic typing.

%it is quite possible that some static type system will work well for this
%however we defer this question.
%interesting variants:
%- require static single method matches
%- reject programs with no-method errors
%- reject programs that yield Unions

%%%%%%


%% The key ingredients:

%% \begin{singlespace}
%% \begin{enumerate}
%% \item Self-describing data model aware of memory layout
%% \item Type tags with nested structure
%% \item A fully-connected type tree
%% \item Dynamic multiple dispatch over all types % including parameters and varargs
%% \item Dataflow type inference
%% \item Automatic code specialization
%% \end{enumerate}
%% \end{singlespace}

%% This list of features may appear somewhat ad-hoc. However, they turn out to be
%% remarkably strongly coupled, and deeply constrained by our ultimate goal.
%% Each of these features has appeared in some form before, but never in a way
%% that fully solves the problems described here.

%% Challenges of this approach (why has this not been done before?)


\section{Contributions}

Our first contribution is a discussion of the nature of technical computing
that suggests which language-level abstractions might best support real use
cases. Our way of looking at the space leads to a novel design.

%sort of language would form a good base for it. it should emphasize
%complex operators and code generation/specialization.
% this includes our real-world experience with building a new
% technical computing environment from the ground up with a large open
% source community.

Second, we introduce the idea of integrated code selection and specialization
as a design goal. This leads to easy-to-use polymorphism, prioritizing
flexibility and performance over nearly all else.

% we design a type system for this. and introduce the ``evaluate softly
% and carry a big subtype relation'' school of thought.

%2 - the idea of integrating specialization and selection, using multiple dispatch
%and semantic subtyping.

% 3 - an explication through elimination of technical computing language features

James Morris eloquently observed that
``One of the most fruitful techniques of language analysis is explication through
elimination. The basic idea is that one explains a linguistic feature by showing
how one could do without it.'' \cite{morris}
The final contribution of this thesis is the application of this approach to features
of technical computing environments that have not been subject to such analysis
before.
% showing our language is able to ``explain'' ...
